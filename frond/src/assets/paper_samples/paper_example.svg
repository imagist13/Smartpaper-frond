<svg width="800" height="600" viewBox="0 0 800 600" xmlns="http://www.w3.org/2000/svg">
  <rect width="800" height="600" fill="white" />
  
  <!-- 论文标题区域 -->
  <rect x="80" y="60" width="640" height="100" fill="#f9fafb" stroke="#e5e7eb" stroke-width="1" />
  <text x="100" y="100" font-family="Arial" font-size="22" font-weight="bold" fill="#111827">
    Attention Is All You Need
  </text>
  <text x="100" y="130" font-family="Arial" font-size="14" fill="#6b7280">
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
  </text>
  
  <!-- 论文摘要 -->
  <rect x="80" y="180" width="640" height="200" fill="#f9fafb" stroke="#e5e7eb" stroke-width="1" />
  <text x="100" y="210" font-family="Arial" font-size="18" font-weight="bold" fill="#111827">
    Abstract
  </text>
  <text x="100" y="240" font-family="Arial" font-size="14" fill="#374151">
    <tspan x="100" dy="0">The dominant sequence transduction models are based on complex recurrent or</tspan>
    <tspan x="100" dy="20">convolutional neural networks that include an encoder and a decoder. The best</tspan>
    <tspan x="100" dy="20">performing models also connect the encoder and decoder through an attention</tspan>
    <tspan x="100" dy="20">mechanism. We propose a new simple network architecture, the Transformer,</tspan>
    <tspan x="100" dy="20">based solely on attention mechanisms, dispensing with recurrence and convolutions</tspan>
    <tspan x="100" dy="20">entirely. Experiments on two machine translation tasks show these models to</tspan>
    <tspan x="100" dy="20">be superior in quality while being more parallelizable and requiring significantly</tspan>
    <tspan x="100" dy="20">less time to train.</tspan>
  </text>
  
  <!-- 模型结构图 -->
  <rect x="80" y="400" width="640" height="160" fill="#f9fafb" stroke="#e5e7eb" stroke-width="1" />
  
  <!-- 左侧编码器 -->
  <rect x="200" y="420" width="120" height="120" fill="#eef2ff" stroke="#c7d2fe" stroke-width="1" />
  <text x="260" y="440" font-family="Arial" font-size="14" font-weight="bold" fill="#4f46e5" text-anchor="middle">Encoder</text>
  
  <rect x="210" y="455" width="100" height="25" rx="3" fill="#e0e7ff" />
  <text x="260" y="472" font-family="Arial" font-size="12" fill="#4338ca" text-anchor="middle">Self-Attention</text>
  
  <rect x="210" y="485" width="100" height="25" rx="3" fill="#e0e7ff" />
  <text x="260" y="502" font-family="Arial" font-size="12" fill="#4338ca" text-anchor="middle">Feed Forward</text>
  
  <text x="260" y="525" font-family="Arial" font-size="10" fill="#6366f1" text-anchor="middle">Nx</text>
  
  <!-- 右侧解码器 -->
  <rect x="480" y="420" width="120" height="120" fill="#f0fdf4" stroke="#bbf7d0" stroke-width="1" />
  <text x="540" y="440" font-family="Arial" font-size="14" font-weight="bold" fill="#16a34a" text-anchor="middle">Decoder</text>
  
  <rect x="490" y="455" width="100" height="25" rx="3" fill="#dcfce7" />
  <text x="540" y="472" font-family="Arial" font-size="12" fill="#15803d" text-anchor="middle">Self-Attention</text>
  
  <rect x="490" y="485" width="100" height="25" rx="3" fill="#dcfce7" />
  <text x="540" y="502" font-family="Arial" font-size="12" fill="#15803d" text-anchor="middle">Enc-Dec Attention</text>
  
  <rect x="490" y="515" width="100" height="25" rx="3" fill="#dcfce7" />
  <text x="540" y="532" font-family="Arial" font-size="12" fill="#15803d" text-anchor="middle">Feed Forward</text>
  
  <!-- 连接线 -->
  <line x1="320" y1="480" x2="480" y2="480" stroke="#9ca3af" stroke-width="2" stroke-dasharray="5,5" />
  <polygon points="470,475 480,480 470,485" fill="#9ca3af" />
  
  <!-- 页码 -->
  <text x="400" y="585" font-family="Arial" font-size="12" fill="#6b7280" text-anchor="middle">1</text>
</svg> 